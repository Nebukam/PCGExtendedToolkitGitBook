---
description: Tracking changes since 0.71
icon: tag
---

# v0.72

{% hint style="info" %}
NOTE : This changelog includes patch changes as well (`0.72.1`, `0.72.2` etc)
{% endhint %}

<details>

<summary>A note on FAB...</summary>

So if you're on the Discord you may have seen the problems I've been having with FAB backend : <mark style="color:red;">PCGEx is bricked</mark>, and [can't be updated anymore](https://forums.unrealengine.com/t/cant-update-product-since-october-49-days-without-fix-so-far/).

It's been **50 days** (_yes, I count them_) the dev team has been unable to fix it. As one might guess I have a lot of non-politically-correct feelings regarding the situation. Maybe it'll be fixed at some point in the future, but frankly I'm loosing faith. It really feels like nobody gives a crap at Epic about their platform and it's absolutely infuriating and demoralizing.

</details>

## Optimization Update

0.72 is primarily an optimization release. If you check out the git history since 0.71, you will see a lot of something-something-refactor or something-something-revamp.

This is because the codebase has reached enough maturity that I can safely isolate and focus on specific aspects of the architecture and logic and it will benefit the entire ecosystem of nodes — and this is great! I've been able to tackle a bunch of stuff that had been bugging me for quite some time.

<details>

<summary>Async Manager Refactor</summary>

<mark style="color:$success;">Faster execution, much smaller walltimes.</mark>

The way PCGEx chunks and distributes the work has been a recurring tech debt riddled with workarounds and ugly fixes — no more! Work is now properly dispatched, tracked, and doesn't rely on the task scheduler pause/tick mechanism to advance completion and output data. On the same track, keeping tabs on loaded resources and getting a hold on them for the duration of the work should no longer be an issue either.

</details>

<details>

<summary>Type Erasure</summary>

<mark style="color:$success;">Cleaner codebase, faster (?) compile times</mark>

In order to deal with the wide variety of attribute types, I was using templating — think of it as code that can auto-expand and copy-paste itself whenever you need it to work with a given type. This was the #1 cause for extreme long compile times as some classes would deal with ordered type pairs for the blend operations : the blending ecosystem would explode into 5000+ generated classes, structs and methods.&#x20;

I've always pushed that back because honestly I didn't knew how to do better, and ultimately it was robust and fast at runtime. I've refactored all of that so it's now "type erased", on paper it adds a bit of overhead, but even with millions of points I haven't been able to notice the slightest different.

If you're feeling curious you can look at the code [here on github](https://github.com/Nebukam/PCGExtendedToolkit/tree/main/Source/PCGExtendedToolkit/Public/Types); it's a lot to take in :D

</details>

<details>

<summary>Delayed Metadata Entries</summary>

<mark style="color:$success;">Faster attribute preparation, gain scales with data size.</mark>

This is something I should've done a long time ago; there's an API in the PCG framework to create placeholder keys for attribute values which I've always eyed but never implemented : no more!

It's implemented at the lowest level; and while some nodes still use the "slow" way of doing things, the worst offender have been heavily optimized. **This benefit any node that writes attribute, as well as clusters, socket sampling etc — anything that generates new points or modify existing ones.**

_It's especially noticeable on big clusters._

</details>

<details>

<summary>ParallelFor</summary>

<mark style="color:$success;">Faster execution for trivial operations</mark>

I'm making a more generous usage of Unreal' `ParallelFor`, it helps with simple nodes and with cluster (again!) processing. I wasn't using it too much before because my internal task manager wasn't robust enough to accommodate it properly.

Note that I've added a default threshold : below 512 elements it's not parallelized because the overhead usually makes it not worth it. I could probably lower it; if you compile from source you can simply edit the macros [here](https://github.com/Nebukam/PCGExtendedToolkit/blob/7341b279d9751b9cfae2728c04ad697ea44f0131/Source/PCGExtendedToolkit/Public/PCGExMT.h#L58).

<figure><img src="../../.gitbook/assets/image (253).png" alt=""><figcaption></figcaption></figure>

</details>

<details>

<summary>Curve LUT</summary>

<mark style="color:$success;">Anywhere there's a curve, there's a LUT!</mark>

I've introduced a new, optional sampling mode everywhere curve are exposed (blending, tensors, remapping etc) — instead of computing the full evaluation on the curve for each processed point, you can now pre-sample said curve with a custom number of samples. When enabled, the current default is 512.

Honestly I have yet to work with an amount of data that makes the difference measurable; which was disappointing given the amount of work it took to migrate to the new format.

</details>

### Tweaks

* Added blending support & union stats to [Broken link](/broken/pages/VpauTmDsUD2qVYEDsi6Q "mention")
* [Broken link](/broken/pages/dX6S4Ca91eSQDb17qgXR "mention")now support clamping, allowing to optionally ensure there's a fixed min/max amount of points selected. _That value can be driven by data attributes!_
* Added Hash and UnsignedHash reduction modes to [Broken link](/broken/pages/ri7yKzETw95jdsg4g0Ak "mention")!
*

    <figure><img src="../../.gitbook/assets/image (42).png" alt=""><figcaption></figcaption></figure>





<mark style="color:$success;">**To be continued!**</mark>
